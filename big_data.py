# -*- coding: utf-8 -*-
"""Big Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qdv98DSshK2YzURGRWsOQXDLLRAPTzP9
"""



import io
import nltk
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize, pos_tag
import numpy as np
import pandas as pd
from nltk.corpus import wordnet as wn
import spacy
from nltk.corpus import stopwords
from nltk.wsd import lesk


def penn_to_wn(tag):
  """ Convert between a Penn Treebank tag to a simplified Wordnet tag """
  if tag.startswith('N'):
    return 'n'

  if tag.startswith('V'):
    return 'v'

  if tag.startswith('J'):
    return 'a'

  if tag.startswith('R'):
    return 'r'

  return None


def task2(sentence):
  # loading spacy model
  nlp = spacy.load("en_core_web_sm")

  # Tokenization
  tokens = []
  tokens = nltk.word_tokenize(sentence);
  print("Tokens: ", tokens)

  # Lemmatization without POS tags
  lems = []
  lemmatizer = WordNetLemmatizer()
  for word in tokens:
    lems.append(lemmatizer.lemmatize(word));
  print("Lemmatization of sentence without POS tags: ", lems)

  # POS Tagging
  pos_sen = pos_tag(tokens);
  print("POS Tags: ", pos_sen);

  pos_wn = [(s[0], penn_to_wn(s[1])) for s in pos_sen]
  print("POS Tags for wordnet: ", pos_wn)

  # Dependency Parse Tree
  sen = nlp(sentence)
  for token in sen:
    print("\n Dependency parse tree: \n")
    print(
      "{2}({3}-{6}, {0}-{5})".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i + 1,
                                     token.head.i + 1))

  # Finding the synsets of the words:
  synset_sen = []
  synset_sen = [wn.synsets(s) for s in sentence]
  print("\n The synsets of all the words in the sentence: \n")
  [print(s, " : ", wn.synsets(s)) for s in tokens]

  # Word Sense Disambiguation
  word_sense = []
  for w in pos_wn:
    if (w[1]):
      word_sense.append((w[0], lesk(sentence, w[0], pos=w[1])))
    else:
      word_sense.append((w[0], lesk(sentence, w[0])))

  print("\n disambiguated word senses: \n")
  print(word_sense)

  # Finding Relations

  hypernyms_sen = []
  hyponyms_sen = []
  holonyms_sen = []
  meronyms_sen = []

  for w in word_sense:
    if (w[1]):
      hypernyms_sen.append((w[0], w[1].hypernyms()))
      hyponyms_sen.append((w[0], w[1].hyponyms()))
      holonyms_sen.append((w[0], w[1].part_holonyms))
      meronyms_sen.append((w[0], w[1].part_meronyms))

    else:
      hypernyms_sen.append((w[0], None))
      hyponyms_sen.append((w[0], None))
      holonyms_sen.append((w[0], None))
      meronyms_sen.append((w[0], None))

  print("\n Hypernyms: \n")
  print(hypernyms_sen)

  print("\n Hyponyms: \n")
  print(hyponyms_sen)

  print("\n Holonyms: \n")
  print(holonyms_sen)

  print("\n Meronyms: \n")
  print(meronyms_sen)

nltk.download('punkt')

nltk.download('wordnet')

nltk.download('averaged_perceptron_tagger')

newPositiveDataSetFile = open("positiveDataset.txt", 'r+')

lines = newPositiveDataSetFile.readlines()
for eachLine in lines:
    task2(eachLine)




